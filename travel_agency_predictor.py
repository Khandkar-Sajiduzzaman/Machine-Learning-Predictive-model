# -*- coding: utf-8 -*-
"""Travel Agency predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mFVAV3bAXaJiSfMFRCVdHoBCXHUatRrS
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix

"""#Loading the Data"""

from google.colab import drive
drive.mount('/content/drive')

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/travel insurance.csv")

# Display info
print(df.shape)
df.head(10)

"""# **Exploratory Data Analysis**"""

df.info()

"""# Categorical Data Analysis"""

#Selecting categoricalfeatures
categorical_data=df.select_dtypes(include= 'object')

#append the features of categorical_data to list
categorical_features=categorical_data.columns.tolist()

print(f'There are {len(categorical_features)} categorical features:', '\n')
print(categorical_features)

# unique values counts
unique_counts=categorical_data.nunique()
print(unique_counts)

"""Checking the distribution of Data"""

for col in categorical_features:
  plt.figure(figsize=(8,8))
  plt.title(f'Distribution of {col}')
  sns.countplot(x=df[col])
  plt.show()

"""Checking the correlation Heatmap"""

import scipy.stats as ss

def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
    rcorr = r - ((r - 1) ** 2) / (n - 1)
    kcorr = k - ((k - 1) ** 2) / (n - 1)
    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))

# Calculate Cramer's V matrix
cramers_v_matrix = pd.DataFrame(
    [[cramers_v(df[col1], df[col2]) for col1 in categorical_features] for col2 in categorical_features],
    columns=categorical_features, index=categorical_features
)

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cramers_v_matrix, annot=True, cmap='Blues')
plt.title('Cramer\'s V Correlation Matrix for Categorical Features')
plt.show()

"""# Numerical Data Analysis"""

##Selecting numerical features
numerical_data = df.select_dtypes(include='number')

#append the features of numerical_data to list
numerical_features=numerical_data.columns.tolist()

print(f'There are {len(numerical_features)} numerical features:', '\n')
print(numerical_features)

"""Plotting Histograms"""

numerical_data.hist(figsize=(8,8))
plt.show()

"""#boxplots"""

for i in numerical_features:
  plt.figure(figsize=(40,4))
  sns.boxplot(x=df[i])
  plt.show()

"""#Numerical_Features Heatmap

"""

# Select numeric columns
num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()

# --- Heatmap vs Agency Type ---
corr_agency = df[num_cols].corr()

plt.figure(figsize=(10,6))
sns.heatmap(corr_agency, cmap = 'YlGnBu')
plt.title("Correlation Heatmap - Numeric Features")
plt.show()

numerical_data.var()

"""## Combined Heatmap

"""

from sklearn.preprocessing import OneHotEncoder

##Selecting numerical features
numerical_data = df.select_dtypes(include='number')

#append the features of numerical_data to list
numeric_features=numerical_data.columns.tolist()

categorical_data=df.select_dtypes(include= 'object')

#append the features of categorical_data to list
categorical_features=categorical_data.columns.tolist()


# One-hot encode categorical features
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_categorical_data = pd.DataFrame(encoder.fit_transform(categorical_data), columns=encoder.get_feature_names_out(categorical_features))

# Combine numerical and encoded categorical data
combined_df = pd.concat([numerical_data, encoded_categorical_data], axis=1)

# Heatmap of numerical and encoded categorical features
plt.figure(figsize=(200, 150))
sns.heatmap(combined_df.corr(), cmap = 'YlGnBu')
plt.title('Correlation Heatmap - Numerical and Encoded Categorical Features')
plt.show()

"""#Handling Null_Values"""

df.isnull().sum() #counting the number of null values in the dataframe

"""#Dropping Majority Null value column"""

df = df.drop(columns=["Gender"])

"""#Feature Engineering

Dropping columns based on correlations
"""

drop_cols = ["Agency", "Product Name"]
df = df.drop(columns=[c for c in drop_cols if c in df.columns])

"""#Categorical Encoding"""

# Define target and features
y = df["Agency Type"].map({"Airlines":0, "Travel Agency":1})  # Binary target
X = df.drop(columns=["Agency Type"])

# Identify categorical and numerical features
categorical_features = X.select_dtypes(include=["object"]).columns.tolist()
numerical_features = X.select_dtypes(include=["int64","float64"]).columns.tolist()

print("Categorical:", categorical_features)
print("Numerical:", numerical_features)

# Preprocessing pipeline
preprocess = ColumnTransformer(transformers=[
    ("num", StandardScaler(), numerical_features),
    ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
])

X_processed = preprocess.fit_transform(X).toarray()

print("Processed feature shape:", X_processed.shape)

"""#Data Splitting"""

X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y, test_size=0.3, stratify=y, random_state=42
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""#KNN"""

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Define candidate K values (odd only, to avoid ties)
k_values = [3, 5, 7, 9, 11]

results = {}

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k, weights="uniform", metric="euclidean")
    scores = cross_val_score(knn, X_processed, y, cv=5, scoring="accuracy", n_jobs=-1)
    results[k] = scores.mean()
    print(f"K={k} → CV Accuracy: {scores.mean():.4f}")

# Find best K
best_k = max(results, key=results.get)
print("\nBest K:", best_k, "with CV Accuracy:", results[best_k])

# Train KNN
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)

# Predict
y_pred_KNN = knn.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred_KNN))
print("\nClassification Report:\n", classification_report(y_test, y_pred_KNN))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_KNN)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Airlines","Travel Agency"],
            yticklabels=["Airlines","Travel Agency"])
plt.title("Confusion Matrix - KNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""#

#Logistic Regression
"""

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
# Logistic Regression from scratch
class LogisticRegressionGD:
    def __init__(self, lr=0.001, epochs=1000):
        self.lr = lr
        self.epochs = epochs

    def fit(self, X, y):
        m, n = X.shape
        self.W = np.zeros(n)
        self.b = 0
        self.losses = []

        for _ in range(self.epochs):
            linear_model = np.dot(X, self.W) + self.b
            y_pred_LR = sigmoid(linear_model)

            # Compute gradients
            dw = (1/m) * np.dot(X.T, (y_pred_LR - y))
            db = (1/m) * np.sum(y_pred_LR - y)

            # Update weights
            self.W -= self.lr * dw
            self.b -= self.lr * db

            # Compute loss
            loss = -(1/m) * np.sum(y*np.log(y_pred_LR+1e-10) + (1-y)*np.log(1-y_pred_LR+1e-10))
            self.losses.append(loss)

    def predict(self, X):
        linear_model = np.dot(X, self.W) + self.b
        y_pred_LR = sigmoid(linear_model)
        return np.where(y_pred_LR >= 0.5, 1, 0)

# Train model
model_Logistic = LogisticRegressionGD(lr=0.1, epochs=1000)
model_Logistic.fit(X_train, y_train)
# Predictions
y_pred_LR = model_Logistic.predict(X_test)

# Accuracy
print("Test Accuracy:", accuracy_score(y_test, y_pred_LR))

print("Logistic Regression:")
print(classification_report(y_test, y_pred_LR))
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_LR)
print("\nConfusion Matrix:\n", cm)

"""#Neural Network"""

model_NN = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(512, activation='relu'),
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(32, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation="sigmoid") # Output layer for binary classification
])

# Compile with Gradient Descent optimizer (SGD)
model_NN.compile(optimizer=SGD(learning_rate=0.001),
              loss="binary_crossentropy",
              metrics=["accuracy"])

model_NN.summary()

# Train model_NN
history = model_NN.fit(X_train, y_train,
                    epochs=20,
                    validation_data=(X_test, y_test),
                    verbose=1)

test_acc = model_NN.evaluate(X_test, y_test, verbose=0)[1]
print(f"\nTest Accuracy: {test_acc*100:.2f}%")
y_pred_NN = (model_NN.predict(X_test, verbose=0).ravel() >= 0.5).astype(int)
print("\nClassification Report:\n", classification_report(y_test, y_pred_NN))

cm = confusion_matrix(y_test, y_pred_NN)
ConfusionMatrixDisplay(cm, display_labels=[0,1]).plot(cmap="Blues")
plt.title("Confusion Matrix – NN (target: Location)")
plt.tight_layout(); plt.show()

# Evaluate on test data
loss, accuracy = model_NN.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {accuracy*100:.2f}%")
print(f"Test Loss: {loss:.4f}")

# Plot loss curve
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss Curve")
plt.show()

# 2) (Optional) accuracy curve (key name varies by TF version)
train_acc_key = 'accuracy' if 'accuracy' in history.history else 'acc'
val_acc_key   = 'val_accuracy' if 'val_accuracy' in history.history else 'val_acc'
if train_acc_key in history.history:
    plt.figure(figsize=(6,4))
    plt.plot(history.history[train_acc_key], label='train acc')
    if val_acc_key in history.history:
        plt.plot(history.history[val_acc_key], label='val acc')
    plt.title('Accuracy per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

"""#Precision, Recall, Accuracy"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Collect predictions into a dict
models_preds = {
    "KNN": y_pred_KNN,
    "Logistic Regression": y_pred_LR,
    "Neural Network": y_pred_NN
}

# Evaluate metrics
results = {}
for name, y_pred in models_preds.items():
    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average="weighted"),
        "Recall": recall_score(y_test, y_pred, average="weighted")
    }

# Convert to DataFrame
metrics_df = pd.DataFrame(results).T
display(metrics_df)

# === 1. Bar chart - Accuracy ===
plt.figure(figsize=(8,5))
sns.barplot(x=metrics_df.index, y=metrics_df["Accuracy"])
plt.title("Prediction Accuracy of Models")
plt.ylabel("Accuracy")
plt.xticks(rotation=45)
plt.show()

# === 2. Precision & Recall Comparison ===
metrics_df[["Precision", "Recall"]].plot(kind="bar", figsize=(10,6))
plt.title("Precision & Recall Comparison")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.show()

# === 3. Confusion Matrices for all models ===
for name, y_pred in models_preds.items():
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"Confusion Matrix – {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

"""#K fold for Neural Network"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

# -------------------------------
# Define NN model
# -------------------------------
def build_nn(input_dim):
    model = Sequential([
        Dense(512, activation='relu', input_shape=(input_dim,)),
        Dense(512, activation='relu'),
        Dense(128, activation='relu'),
        Dense(128, activation='relu'),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(64, activation='relu'),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(32, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1, activation="sigmoid")  # Output for binary classification
    ])

    # Compile with SGD optimizer
    model.compile(optimizer=SGD(learning_rate=0.001),
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
    return model

# -------------------------------
# Stratified K-Fold CV
# -------------------------------
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
accs = []

X_array = np.array(X_processed)
y_array = np.array(y)

for fold, (train_idx, test_idx) in enumerate(cv.split(X_array, y_array), 1):
    X_train, X_test = X_array[train_idx], X_array[test_idx]
    y_train, y_test = y_array[train_idx], y_array[test_idx]

    model = build_nn(X_train.shape[1])
    model.fit(X_train, y_train, epochs=20, verbose=1)

    y_pred = (model.predict(X_test) >= 0.5).astype(int).ravel()
    acc = accuracy_score(y_test, y_pred)
    accs.append(acc)
    print(f"Fold {fold} accuracy: {acc:.4f}")

print(f"\nMean CV Accuracy: {np.mean(accs):.4f}")

"""#K-Fold for KNN"""

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Define candidate K values (odd only, to avoid ties)
k_values = [3, 5, 7, 9, 11]

results = {}

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k, weights="uniform", metric="euclidean")
    scores = cross_val_score(knn, X_processed, y, cv=5, scoring="accuracy", n_jobs=-1)
    results[k] = scores.mean()
    print(f"K={k} → CV Accuracy: {scores.mean():.4f}")

# Find best K
best_k = max(results, key=results.get)
print("\nBest K:", best_k, "with CV Accuracy:", results[best_k])
# Use already processed dense features
X_array = np.array(X_processed)
y_array = np.array(y)

# Define KNN model
knn = KNeighborsClassifier(n_neighbors=best_k)

# Stratified K-Fold CV
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Cross-validation scores
scores = cross_val_score(knn, X_array, y_array, cv=cv, scoring="accuracy")

print("KNN accuracy per fold:", scores)
print("Mean accuracy:", scores.mean())

"""#K-Fold for Logistic Regression"""

from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# Convert to NumPy arrays
X_array = np.array(X_processed)
y_array = np.array(y)

kf = KFold(n_splits=3, shuffle=True, random_state=42)

accuracies, precisions, recalls, f1s, conf_matrices = [], [], [], [], []
fold = 1

for train_index, test_index in kf.split(X_array):
    X_train, X_test = X_array[train_index], X_array[test_index]
    y_train, y_test = y_array[train_index], y_array[test_index]

    # fresh model each fold
    model = LogisticRegressionGD()
    model.fit(X_train, y_train)

    # predictions
    y_pred = model.predict(X_test)

    # metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    # save results
    accuracies.append(acc)
    precisions.append(prec)
    recalls.append(rec)
    f1s.append(f1)
    conf_matrices.append(confusion_matrix(y_test, y_pred))

    print(f"Fold {fold} → Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}")
    fold += 1

print(f"Mean Accuracy : {np.mean(accuracies):.4f}")

"""#AUC,ROC"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

plt.figure(figsize=(10, 8))

# --- Logistic Regression (from scratch GD) ---
log_reg = LogisticRegressionGD(lr=0.1, epochs=1000)
log_reg.fit(X_train, y_train)

# manually compute probabilities using sigmoid
linear_model = np.dot(X_test, log_reg.W) + log_reg.b
y_probs_logreg = 1 / (1 + np.exp(-linear_model))

fpr, tpr, _ = roc_curve(y_test, y_probs_logreg)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f"Logistic Regression (AUC = {roc_auc:.2f})")

# --- Neural Network (Keras model) ---
nn_model = build_nn(X_train.shape[1])
nn_model.fit(X_train, y_train, epochs=10)
y_probs_nn = nn_model.predict(X_test).ravel()

fpr, tpr, _ = roc_curve(y_test, y_probs_nn)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f"Neural Net (AUC = {roc_auc:.2f})")

# --- KNN ---
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_probs_knn = knn.predict_proba(X_test)[:,1]

fpr, tpr, _ = roc_curve(y_test, y_probs_knn)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f"KNN (AUC = {roc_auc:.2f})")

# --- Plot random baseline ---
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')

# --- Finalize plot ---
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison - Travel Insurance Dataset")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""#K-mean clustering"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from kneed import KneeLocator
from sklearn.decomposition import PCA

# --- Step 1: Range of K values ---
K_range = range(1, 11)
sse = []

# --- Step 2: Compute SSE (Inertia) for each K ---
for k in K_range:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_processed)
    sse.append(km.inertia_)

# --- Step 3: Find elbow automatically ---
kl = KneeLocator(K_range, sse, curve="convex", direction="decreasing")
best_k = kl.knee
print(f"Optimal K (Elbow Point) = {best_k}")

# --- Step 4: Fit KMeans with optimal K ---
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_processed)

# --- Step 5: PCA for 2D visualization ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_processed)
centers_pca = pca.transform(kmeans.cluster_centers_)

# --- Step 6: Plot clusters with centroids ---
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap="tab10", s=40, alpha=0.7)
plt.scatter(centers_pca[:,0], centers_pca[:,1], c="black", marker="X", s=200, label="Centroids")

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title(f"KMeans Clustering with Optimal K={best_k}")
plt.legend()
plt.grid(True)
plt.show()

# --- Step 7: Plot Elbow Curve ---
plt.figure(figsize=(8,6))
plt.plot(K_range, sse, marker="o", linestyle="--", label="SSE")
plt.vlines(best_k, plt.ylim()[0], plt.ylim()[1], linestyles="dashed", colors="red", label=f"Elbow at K={best_k}")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("SSE (Inertia)")
plt.title("Elbow Method with Automatic Elbow Detection")
plt.legend()
plt.grid(True)
plt.show()